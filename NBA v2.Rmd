---
title: "Lebron James vs Michael Jordan: Who is the G.O.A.T ?"
author: "Alvaro Polo García y Nicolas García Valdeande"
date: "3/12/2021"
output:
  word_document: default
  pdf_document: default
  html_document:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: true
      smooth_scroll: true
editor_options: 
  chunk_output_type: inline
---

![](https://www.blogdebasket.com/files/michael-jordan-lebron-james.jpg)


From the dawn of time, NBA fans have asked themselves on question, who is the Greatest (Basketball Player) of all Time? Decades of discussion have led nowhere, as there still isn't a consensus on who the GOAT is.

This report will try and end the debate and statistically calculate who is the greatest basketball player of all time. In essence, we will reduce all the statistics into one number...


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
##Importar los packages
library(tidyverse)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

# Exploratory Data Analysis

Firstly, we have to load the database, which we found on kaggle (meter el link), it includes statistics for every player to play a game since 1976.
```{r}

library(tidyverse)

nba = read.csv("./Data/NBA Season Data.csv")

```

Firstly, we must conduct a preliminary analysis on the data set. As the dataset is of great extent, we mut choose what variables will be included in the analysis. We have looked at all the variables, and with our basketball knowledge, have decided that these are the most relevant variables. The rest of the variable will not be included in the preliminary analysis or model. With the little processing power we have at our disposal, it wasn't viable to include all 109 variables.

```{r}

nba = nba %>% 
  select(Player.ID, Player, Year, Age, G, MP, PER, TS., TRB., AST., STL., X3PAr, BLK., TOV., VORP, Shot., Height, Rounded.Position, Weight, Yrs.Experience, WS)

head(nba)

```

```{r}

str(nba)

```

We transform the variables from type "char" to a "factor" and from "int to "numeric".

```{r}

nba$Weight = as.numeric(nba$Weight)
nba$Height = as.numeric(nba$Height)
nba$Yrs.Experience = as.numeric(nba$Yrs.Experience)
nba$Year= as.numeric(nba$Year)
nba$Age = as.numeric(nba$Age)
nba$MP = as.numeric(nba$MP)
nba$G = as.numeric(nba$G)

nba$Rounded.Position = as.factor(nba$Rounded.Position)
nba$Player.ID = as.factor(nba$Player.ID)
nba$Player = as.factor(nba$Player)


str(nba)

```

Next, we check the dimensions of the data set.

```{r}

dim(nba)

```

We then check to see how many NA values we have in the data set. If there are any, we are going to have to either transform them or delete them.

```{r}

apply(is.na(nba),2,sum)

```
We can observe that there are some NA values, but we can explain most of these. Given that the data set was posted during the 2016 season, most of the data from 2016 is incomplete (NA). Therefore, we will have to do something about this, as we can't have NA values in our model. In addition, there are 718 missing values for 3 point attempts. This is due to the 3 point line to being introduced until 1979, therefore we have some years where that statistic could not be calculated.

Most of the NA values are from the 2016 season, so we our model will not take into account 2016 (Unfortunate as LeBron had an all-time great year, winning the Finals with the Cavs).

```{r}

nba = nba %>% 
  filter(Year != "2016") 

tail(nba)

```

```{r}

nba = na.omit(nba)

```

We check again if there are any NA values, after deleting the 2016 season stats.

```{r}

any(is.na(nba))

```

Given that we are trying to figure out who is the NBA GOAT, it does not make any sense to include in the model players that have played, on average, less than 5 minutes a game in a season. Although statistically it could be possible, in real life it would not make any sense that the best player in NBA history plays less than 5 minutes a game, from a possible 48 minutes. For this reason, we will be ignoring these observations (28% of the data set).


```{r}

nba = nba %>% 
  filter(MP > 400)

```

We also want a Minutes Per Game (MPG) variable, as it is easier to work with, when compared to Total Minutes Played (MP) and Games Played (G).

```{r}

nba = nba %>% 
  mutate(MPG = MP/G) %>% 
  select(-MP, -G) 

head(nba)

```

We are now going to start to build our multivariate linear regression model. 

Firstly, we must decide the output variable. It is common to use the Points per Game (PPG) variable when deciding who the best player is (similar thing happens with football, fans compare the amount of goals scored), but basketball is much more than just scoring points. If we selected PPG, then we would not take into account defence, rebounding and, playmaking, all key attributes to become the GOAT. So, instead of using PPG, we have to chosen the Win Share (WS). WS is a player statistic which attempts to divvy up credit for team success to the individuals on the team, measuring the impact the player has on the number of wins. The higher the WS variable is, the more a player contributes to the teams win.


Now we are going explore the relationship between WS and the other variables we have selected. If they have some sort of relationship, they could be included in the final model.

**Relation between MPG (Minutes Per Game) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = MPG), col = "blue")+
  ggtitle("Comparison WS and MPG ") 

```

**Relation between PER (Player efficiency rate) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = PER), col = "blue")+
  ggtitle("Comparison WS and PER ") 

```

**Relation between TS.(True shooting) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = TS.), col = "blue")+
  ggtitle("Comparison WS and TS ") 

```

**Relation between TRB.(Total Rebounds) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = TRB.), col = "blue")+
  ggtitle("Comparison WS and TRB") 

```


**Relation between AST.(Assists) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = AST.), col = "blue")+
  ggtitle("Comparison WS and AST ") 

```

**Relation between STL.(Steal%) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = STL.), col = "blue")+
  ggtitle("Comparison WS and TS ") 

```


**Relation between X3PAR(X3 attemps %) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = X3PAr), col = "blue")+
  ggtitle("Comparison WS and X3PAR ") 

```

**Relation between BLK(BLOCKS %) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = BLK.), col = "blue")+
  ggtitle("Comparison WS and BLK") 

```

**Relation between TOV.(Balls lost %) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = TOV.), col = "blue")+
  ggtitle("Comparison WS and TOV ") 

```

**Relation between VORP.(Value over replacement player) and WS**

```{r}

ggplot(data = nba) +
  geom_point(mapping = aes(x = WS, y = VORP), col = "blue")+
  ggtitle("Comparison WS and VORP ") 

```


After looking at all these relationships, we are going to calculate the correlation WS has with the other variables in order to find out which ones show a strong relationship.
 

**Correlagram**
```{r message=FALSE, warning=FALSE}

library(GGally)

ggcorr(nba, layout.exp = 2) + labs(title='Correlogram of quantitative variables')

```

We observe that the variables that VORP, TS and PER have a strong correlation with WS.
  
Taking into account the correlogram of quantitative variables, we can also obtain the numerical correlation matrix.

```{r}

library(corrplot)

nba_gg <- nba%>%
  select(WS, VORP, MPG, PER, TS., TOV.)

correlacion<-cor(nba_gg)

corrplot(correlacion, method = "number", type = "upper")


```

As hypothesized before, we observe that the correlation between WS and VORP is very strong (0.91). The correlation between WS and TS is not as strong as previously thought.


We going to add the age variable into 3 categories. Young (ages between 18 and 24), Prime (ages between 25 and 32) and Experienced (ages from 33 on wards).
```{r}

v1 = nba$Age

v1_cut = cut(v1, breaks = c(min(v1), 24, 32, max(v1)), labels = c("YOUNG", "PRIME", "EXPERIENCED"))
v1_cut[1:5]

```

```{r}

nba = nba %>% 
  mutate(age_Cat = v1_cut) 

head(nba)

```

Graphical analysis between the correlation of Ws/AGe and WS/POs.

```{r}

nba = na.omit(nba) 

library(ggpubr)

p1 <- ggplot(data = nba) +
      geom_boxplot(mapping = aes(x = age_Cat, y = WS))+
      labs(x = "Age")+
      theme_bw()

p2 <- ggplot(data = nba) +
      geom_boxplot(mapping = aes(x = Rounded.Position, y = WS))+
      labs(x = "Position")+
      theme_bw()

final_plot <- ggarrange(p1, p2)

final_plot


```

We can clearly see how the position does not have a correlation with Ws. On the other hand we can see how players in their prime do influence thw WS more, than the other 2 age categories



# Model Building

Predicting the WS with all the other variables.
This report will focus the analysis on two main players, LeBron James and Michael Jordan. Although there is no consensus regarding who is the GOAT, there is more consensus that the debate is mainly between these 2 players.

Therefore, we exclude their observations from the model, as we do not to influence the model.

```{r}

nba_model = nba %>% 
  filter(Player != "Michael Jordan" & Player != "LeBron James") %>% 
  select(-Player.ID, -Year, -Player, -Age, -age_Cat, -Rounded.Position) 

head(nba_model)

```

## Multivariate linear regression model

Checking to see if we have any outliers. Most of the outliers we encounter will be statistical outliers, but we can't delete them, as some players are just a lot better than the average.
```{r}

library(ggpubr)

p1 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = PER)) +
      labs(x = "PER")+
      theme_bw() 

p2 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = TS.))+
      labs(x = "TS")+
      theme_bw() 

p3 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = TRB.))+
      labs(x = "TRB.")+
      theme_bw() 

p4 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = AST.))+
      labs(x = "AST.")+
      theme_bw()

p5 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = STL.))+
      labs(x = "STL.")+
      theme_bw()

p6 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = X3PAr)) +
      labs(x = "XEPAr")+
      theme_bw() 

p7 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = BLK.)) +
      labs(x = "BLK.")+
      theme_bw() 

p8 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = TOV.)) +
      labs(x = "TOV.")+
      theme_bw() 

p9 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = VORP)) +
      labs(x = "VORP.")+
      theme_bw() 

p10 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = Shot.)) +
      labs(x = "Shot.")+
      theme_bw() 

p11 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = Height)) +
      labs(x = "Height")+
      theme_bw() 

p12 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = Weight)) +
      labs(x = "Weight")+
      theme_bw() 

p13 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = Yrs.Experience)) +
      labs(x = "Yrs.Experience")+
      theme_bw() 

p14 <- ggplot(data = nba_model) +
      geom_boxplot(mapping = aes(x = MPG)) +
      labs(x = "MPG")+
      theme_bw() 


final_plot <- ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14)

final_plot

```

In the above graph, we can see how there are some players whose Height and Weight equal 0. It is physically impossible to not weigh anything or to be 0 inches tall, therefore we will ignore these values. 


```{r}

nba_model = nba_model %>% 
  filter(Weight > 0 & Height > 0)

```


```{r}

str(nba_model)

```
We then move on to calculate the Pearson Correlation matrix, given that most of our variables have a linear relationship.

```{r}

round(cor(x = nba_model, method = "pearson"), 3)

```

We build our first model with all our variables.
```{r}

modelo <- lm(WS ~ ., data = nba_model )
summary(modelo)

```
There are several variables that are not statistically significant, those with a p-value larger than 0.05. We use the stepwise variable selection method to include only statistically significant variables.

```{r}

step(object = modelo, direction = "both", trace = 1)

```

Here we include only the statistically significant variables in the model

```{r}

modelo <- (lm(formula = WS ~ PER+TS.+TRB.+AST.+STL.+X3PAr+BLK.+TOV.+VORP+Height+Yrs.Experience+MPG+Shot.+Weight, data = nba_model))
summary(modelo)

```

But, we realize that Weight and Years of experience are not statistically significant, so we ignore them in the model. Once we do this, TS becomes significant.

```{r}

modelo <- (lm(formula = WS ~ PER+TS.+TRB.+AST.+STL.+X3PAr+BLK.+TOV.+VORP+Height+MPG+Shot., data = nba_model))
summary(modelo)

```


From this model, we are able to calculate 90% of the WS. In other words, we can estimate very accurately the WS of a new player, given all other statistics included in the model.

Coefficient 95% confidence interval.

```{r}

confint(lm(formula = WS ~ PER+TS.+TRB.+AST.+STL.+X3PAr+BLK.+TOV.+VORP+Height+MPG, data = nba_model))

```

## Model residual analysis


```{r}

library(gridExtra)

plot1 <- ggplot(data = nba_model, aes(PER, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot2 <- ggplot(data = nba_model, aes(TS., modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot3 <- ggplot(data = nba_model, aes(TRB., modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot4 <- ggplot(data = nba_model, aes(AST., modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot5 <- ggplot(data = nba_model, aes(STL., modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot6 <- ggplot(data = nba_model, aes(X3PAr, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot7 <- ggplot(data = nba_model, aes(BLK., modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot8 <- ggplot(data = nba_model, aes(TOV., modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot9 <- ggplot(data = nba_model, aes(VORP, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot10 <- ggplot(data = nba_model, aes(MPG, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot11 <- ggplot(data = nba_model, aes(Height, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, plot10, plot11)


```

We can observe that the residuals satisfy the classical assumptions. They:
  * The error term has mean 0 (red line)
  * There is no relationship between the model residuals and the variables
  * The error term is homoscedastic (has constant variance)

```{r}

qqnorm(modelo$residuals)
qqline(modelo$residuals)

```

We can also see how our model has a normal distribution (another classical assumption).

We then proceed to analyse the model fitted values with the residuals.
```{r}

ggplot(data = nba_model, aes(modelo$fitted.values, modelo$residuals)) +
geom_point() +
geom_smooth(color = "firebrick", se = FALSE) +
geom_hline(yintercept = 0) +
theme_bw()

```

The mean is close to 0, although it does deviate slightly when the fitted values are negative, suggesting we can lose some predicting capabilities, if the WS is negative. This theoretically should not be a problem, as we expected LeBron James and Michael Jordan to have large positive WS values.

```{r}

library(corrplot)
corrplot(cor(dplyr::select(nba_model, PER, TS., TRB., AST., STL., X3PAr, BLK., TOV., VORP, Height, Yrs.Experience, MPG)),
         method = "number", tl.col = "black")

```

We then also check whether there is multicollinearity present in our model.

```{r}

library(car)
vif(modelo)

```
We observe that PER than a Variance Inflation Factor (VIF) of 20, suggesting there is multicollinearity present. 

Given that PER has a very large VIF, we expect to be able to predict PER with the rest of the variables in the model, therefore we test the hypothesis.

```{r}

modelo_per = lm(formula = PER ~ ., data = nba_model)
summary(modelo_per)

```
With the rest of the variables, we are able to predict 95% of PER, therefore it is not necessary to include it in the model.

New linear model without PER.

```{r}

modelo_bueno <- (lm(formula = WS ~ TS.+TRB.+AST.+STL.+X3PAr+BLK.+TOV.+VORP+Height+MPG, data = nba_model))
summary(modelo_bueno)


```

We can explain 89.97% of a players WS with the rest of the variables.

Confidence interval for the new model.

```{r}

confint(lm(formula = WS ~ TS.+TRB.+AST.+STL.+X3PAr+BLK.+TOV.+VORP+Height+MPG, data = nba_model))

```


```{r}

library(gridExtra)

plot2 <- ggplot(data = nba_model, aes(TS., modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot3 <- ggplot(data = nba_model, aes(TRB., modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot4 <- ggplot(data = nba_model, aes(AST., modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot5 <- ggplot(data = nba_model, aes(STL., modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot6 <- ggplot(data = nba_model, aes(X3PAr, modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot7 <- ggplot(data = nba_model, aes(BLK., modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot8 <- ggplot(data = nba_model, aes(TOV., modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot9 <- ggplot(data = nba_model, aes(VORP, modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot10 <- ggplot(data = nba_model, aes(MPG, modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

plot11 <- ggplot(data = nba_model, aes(Height, modelo_bueno$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()

grid.arrange(plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, plot10, plot11)


```

We can observe that the residuals still satisfy all the classical assumptions, as before.

```{r}

qqnorm(modelo_bueno$residuals)
qqline(modelo_bueno$residuals)

```

Model also follows a Normal Distribution

```{r}

ggplot(data = nba_model, aes(modelo_bueno$fitted.values, modelo_bueno$residuals)) +
geom_point() +
geom_smooth(color = "firebrick", se = FALSE) +
geom_hline(yintercept = 0) +
theme_bw()

```

But, the model still does not do well when predicting negative WS values.

We check to see if we still have some multicollinearity present.

```{r}

library(car)
vif(modelo_bueno)

```

All VIF values are low, suggesting no multicollinearity present.

We proceed to check whether there is any autocorrelation present, the closer the D-W Stat is to 2, the less autocorrelation there is.
```{r}

library(car)
dwt(modelo_bueno, alternative = "two.sided")

```
The D-w is 1.7, very close to 2, so we confirm there is no autocorrelation present.


Studentized residual plot. 

```{r}

library(dplyr)
nba_model$studentized_residual <- rstudent(modelo_bueno)
ggplot(data = nba_model, aes(x = predict(modelo_bueno), y = abs(studentized_residual))) +
geom_hline(yintercept = 3, color = "grey", linetype = "dashed") +
# se identifican en rojo observaciones con residuos estandarizados absolutos > 3
geom_point(aes(color = ifelse(abs(studentized_residual) > 3, 'red', 'black'))) +
scale_color_identity() +
labs(title = "Distribución de los residuos studentized",
     x = "predicción modelo") + 
theme_bw() + theme(plot.title = element_text(hjust = 0.5))

```

We can observe that we have some outliers in the prediction of the model, but these are a very small amount relative to the data set. 

Only 100 values!
```{r}

which(abs(nba_model$studentized_residual) > 3)

```

# Comparison between LeBron James and Michael Jordan

We calculate which is LeBron's best statistical year.
```{r}

Lebron_stats = nba %>% 
  filter(Player == "LeBron James") %>% 
  select(Year, TS., TRB., AST., STL., X3PAr, BLK., TOV., VORP, Height, MPG) %>% 
  mutate(Suma_stats = rowSums(.[setdiff(names(.),"Year")])) %>% 
  arrange(desc(Suma_stats))

Lebron_stats

```
Result = 2010

```{r}

Lebron_2010 = Lebron_stats %>% 
  filter(Year == 2010)

Lebron_2010

```
Calculate Jordan's best statistical year
```{r}

Michael_stats = nba %>%
  filter(Player == "Michael Jordan") %>% 
  select(Year, TS., TRB., AST., STL., X3PAr, BLK., TOV., VORP, Height, MPG) %>% 
  mutate(Suma_stats = rowSums(.[setdiff(names(.),"Year")])) %>% 
  arrange(desc(Suma_stats))

Michael_stats
  
```
Result = 1989

```{r}

Michael_1989 = Michael_stats %>% 
  filter(Year == 1989)

Michael_1989

```

We have chosen to analyze what player was the best in their prime, therefore we only choose a singular year. Later on we will take into account their whole careers.

We continue to predict LeBron's WS with his 2010 stats
```{r}

predict(modelo_bueno, newdata = Lebron_2010)

```
According to our model, LeBron would have a WS contribution of 20.28.

Predicting Jordan's WS.
```{r}

predict(modelo_bueno, newdata = Michael_1989)

```
Our model predicts that Jordan's contribution to the WS would be 21.62, significantly higher than LeBron James. 

# Comparing their best year stats.

```{r}

df_compare = merge(x = Lebron_2010, y = Michael_1989, all = TRUE)

df_compare$Year <- if_else(df_compare$Year == 2010, "Lebron", "Jordan")

df_compare = rename(df_compare, c("Player" = "Year"))
                    
head(df_compare)

```

Visual comparison of their stats.
```{r}

library(ggpubr)


p1 <- ggplot( data = df_compare, aes(Player, AST., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "ASSISTS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5))
      

p2 <- ggplot( data = df_compare, aes(Player, VORP, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "VALUE OVER REPLACEMENT")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p3 <- ggplot( data = df_compare, aes(Player, TS., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "TRUE SHOOTING PERCENTAGE")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p4 <- ggplot( data = df_compare, aes(Player, TOV., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "TURN OVERS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5))

p5 <- ggplot( data = df_compare, aes(Player, Height, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "HEIGHT")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5))

p6 <- ggplot( data = df_compare, aes(Player, TRB., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "TOTAL REBOUND PERCENTAGE")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p7 <- ggplot( data = df_compare, aes(Player, STL., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "STEALS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p8 <- ggplot( data = df_compare, aes(Player, X3PAr, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "3 POINT ATTEMPT RATE")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p9 <- ggplot( data = df_compare, aes(Player, BLK., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "BLOCKS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p10 <- ggplot( data = df_compare, aes(Player, MPG, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "MINUTES PER GAME")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 


final_plot1 <- ggarrange(p1, p2, p3, p4)
final_plot2 <- ggarrange(p5, p6, p7, p8)
final_plot3 <- ggarrange(p9, p10)

final_plot1

```

Both players have very similar stats, although LeBron does provide a higher percentage of the teams total assists, when compared to Jordan. LeBron has always been a very complete player, being able to score, assist and rebound effectively, while Jordan has scored more and defended better. Therefore it is not strange to see that LeBron provides a higher percentage of assists. 

```{r}

final_plot2

```

It is also very interesting to see the 3 point attempt rate difference. In the past decade, the NBA has evolved greatly, before teams looked to score two-pointers and generally did not shoot the 3 that much, while nowadays, most teams look for 3 pointers. This explains the difference in the 3 point attempt rate.

```{r}

final_plot3

```

LeBron also does average more blocks, mainly due to his large frame and size. 


# Career Comparison

We will now look at both players entire careers.

We select Jordan's stats.
```{r}

Michael_stats_AVG = nba %>%
  filter(Player == "Michael Jordan") %>% 
  select(Year, TS., TRB., AST., STL., X3PAr, BLK., TOV., VORP, Height, MPG) %>% 
  colMeans()

Michael_stats_AVG_df <- as.data.frame(Michael_stats_AVG)

Michael_stats_AVG_df <- data.frame(t(Michael_stats_AVG_df))


```

And now LeBron's.
```{r}

Lebron_stats_AVG = nba %>% 
  filter(Player == "LeBron James") %>% 
  select(Year, TS., TRB., AST., STL., X3PAr, BLK., TOV., VORP, Height, MPG) %>% 
  colMeans()

Lebron_stats_AVG_df <- as.data.frame(Lebron_stats_AVG)

Lebron_stats_AVG_df <- data.frame(t(Lebron_stats_AVG_df))

```

Statistical comparison between their whole careers.
```{r}

df_compare_AVG = merge(x = Lebron_stats_AVG_df, y = Michael_stats_AVG_df, all = TRUE)

df_compare_AVG$Year <- if_else(df_compare_AVG$Year == 2009.5, "Lebron", "Jordan")

df_compare_AVG = rename(df_compare_AVG, c("Player" = "Year"))
                    
head(df_compare_AVG)


```

We will now try to predict who has been more constant during their whole career.

Predicting LeBron's career WS.
```{r}

predict(modelo_bueno, newdata = Lebron_stats_AVG_df)

```
Career WS = 16.3


Predicting Jordan's career WS.
```{r}

predict(modelo_bueno, newdata = Michael_stats_AVG_df)

```
MJ Career WS = 13.5

While Jordan contributed more towards his WS than LeBron James, during their whole career, LeBron has managed to maintain WS stat.

# Career Statistical Analysis

```{r}

library(ggpubr)


p11 <- ggplot( data = df_compare_AVG, aes(Player, AST., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "ASSISTS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5))
      

p12 <- ggplot( data = df_compare_AVG, aes(Player, VORP, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "VALUE OVER REPLACEMENT")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p13 <- ggplot( data = df_compare_AVG, aes(Player, TS., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "TRUE SHOOTING PERCENTAGE")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p14 <- ggplot( data = df_compare_AVG, aes(Player, TOV., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "TURN OVERS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5))

p15 <- ggplot( data = df_compare_AVG, aes(Player, Height, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "HEIGHT")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5))

p16 <- ggplot( data = df_compare_AVG, aes(Player, TRB., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "TOTAL REBOUND PERCENTAGE")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p17 <- ggplot( data = df_compare_AVG, aes(Player, STL., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "STEALS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p18 <- ggplot( data = df_compare_AVG, aes(Player, X3PAr, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "3 POINT ATTEMPT RATE")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p19 <- ggplot( data = df_compare_AVG, aes(Player, BLK., fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "BLOCKS")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 

p20 <- ggplot( data = df_compare_AVG, aes(Player, MPG, fill=Player) ) +
      geom_bar(stat = "identity", position = position_dodge()) +
      labs(title = "MINUTES PER GAME")+ 
      theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 


final_plot4 <- ggarrange(p11, p12, p13, p14)
final_plot5 <- ggarrange(p15, p16, p17, p18)
final_plot6 <- ggarrange(p19, p20)

final_plot4

```

```{r}

final_plot5

```

```{r}

final_plot6

```

The main difference between their career stats is the 3 point attempt rate (mainly due to the evolution of the NBA).

# Final Conclusion

This report has aimed to finish the NBA GOAT debate by providing a statistically rigorous conclusion.

From our analysis, we can prove that Michael Jordan was better than LeBron James in their respective best years, by nearly contributing one WS more. On the other hand, if we look at their whole careers, LeBron performs much better than Michael Jordan.

Maybe the question we are trying to answer is not the correct one. We should try to ask a more specific question. If we were to choose one player to play for one season in our team, then we would chose Michael Jordan. Nevertheless, if we were to choose a player to play in our team for their whole career, a better option would be LeBron James. 

